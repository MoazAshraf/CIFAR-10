{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "INPUT_SHAPE = [32, 32, 3]\n",
    "N_INPUT_FEATURES = 32 * 32 * 3\n",
    "N_OUTPUT_CLASSES = 10\n",
    "\n",
    "def create_dnn_model(n_hidden=20, n_neurons=100, activation=\"elu\", kernel_initializer=\"he_normal\", lr=0.001,\n",
    "                     use_batch_norm=False, flatten=True):\n",
    "    \"\"\"\n",
    "    Creates a dense neural network\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    if flatten:\n",
    "        model.add(keras.layers.Flatten(input_shape=INPUT_SHAPE))\n",
    "    else:\n",
    "        model.add(keras.layers.InputLayer(input_shape=[N_INPUT_FEATURES]))\n",
    "    if use_batch_norm:\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # add hidden layers\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=activation, kernel_initializer=kernel_initializer))\n",
    "        if use_batch_norm:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    # output layer\n",
    "    model.add(keras.layers.Dense(N_OUTPUT_CLASSES, activation=\"softmax\"))\n",
    "\n",
    "    # configure the model for training\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"],\n",
    "                  optimizer=keras.optimizers.Nadam(learning_rate=lr))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# load the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a validation set\n",
    "X_train_small, X_val, y_train_small, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "LOG_ROOT = 'train_logs'\n",
    "\n",
    "def generate_log_dir(model_name):\n",
    "    run_id = time.strftime(f\"{model_name}_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(LOG_ROOT, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 40000 samples, validate on 10000 samples\nEpoch 1/1000\n40000/40000 - 12s - loss: 5.5842 - accuracy: 0.1652 - val_loss: 2.2526 - val_accuracy: 0.1906\nEpoch 2/1000\n40000/40000 - 9s - loss: 2.1346 - accuracy: 0.2286 - val_loss: 2.0641 - val_accuracy: 0.2432\nEpoch 3/1000\n40000/40000 - 9s - loss: 2.0065 - accuracy: 0.2670 - val_loss: 1.9579 - val_accuracy: 0.2883\nEpoch 4/1000\n40000/40000 - 9s - loss: 1.9329 - accuracy: 0.2944 - val_loss: 1.9297 - val_accuracy: 0.3004\nEpoch 5/1000\n40000/40000 - 9s - loss: 1.8681 - accuracy: 0.3178 - val_loss: 1.8475 - val_accuracy: 0.3380\nEpoch 6/1000\n40000/40000 - 9s - loss: 1.8211 - accuracy: 0.3372 - val_loss: 1.8262 - val_accuracy: 0.3478\nEpoch 7/1000\n40000/40000 - 9s - loss: 1.7750 - accuracy: 0.3544 - val_loss: 1.7791 - val_accuracy: 0.3562\nEpoch 8/1000\n40000/40000 - 9s - loss: 1.7384 - accuracy: 0.3683 - val_loss: 1.7690 - val_accuracy: 0.3637\nEpoch 9/1000\n40000/40000 - 9s - loss: 1.7056 - accuracy: 0.3849 - val_loss: 1.7052 - val_accuracy: 0.3833\nEpoch 10/1000\n40000/40000 - 9s - loss: 1.6735 - accuracy: 0.3930 - val_loss: 1.6894 - val_accuracy: 0.3904\nEpoch 11/1000\n40000/40000 - 9s - loss: 1.6478 - accuracy: 0.4044 - val_loss: 1.6697 - val_accuracy: 0.3980\nEpoch 12/1000\n40000/40000 - 9s - loss: 1.6235 - accuracy: 0.4141 - val_loss: 1.6644 - val_accuracy: 0.4074\nEpoch 13/1000\n40000/40000 - 9s - loss: 1.6021 - accuracy: 0.4241 - val_loss: 1.6523 - val_accuracy: 0.4087\nEpoch 14/1000\n40000/40000 - 9s - loss: 1.5796 - accuracy: 0.4327 - val_loss: 1.6477 - val_accuracy: 0.4092\nEpoch 15/1000\n40000/40000 - 9s - loss: 1.5583 - accuracy: 0.4366 - val_loss: 1.6356 - val_accuracy: 0.4130\nEpoch 16/1000\n40000/40000 - 9s - loss: 1.5419 - accuracy: 0.4453 - val_loss: 1.6244 - val_accuracy: 0.4245\nEpoch 17/1000\n40000/40000 - 9s - loss: 1.5288 - accuracy: 0.4509 - val_loss: 1.6223 - val_accuracy: 0.4219\nEpoch 18/1000\n40000/40000 - 9s - loss: 1.5102 - accuracy: 0.4543 - val_loss: 1.6124 - val_accuracy: 0.4257\nEpoch 19/1000\n40000/40000 - 9s - loss: 1.5000 - accuracy: 0.4611 - val_loss: 1.6213 - val_accuracy: 0.4227\nEpoch 20/1000\n40000/40000 - 9s - loss: 1.4846 - accuracy: 0.4654 - val_loss: 1.6055 - val_accuracy: 0.4318\nEpoch 21/1000\n40000/40000 - 9s - loss: 1.4722 - accuracy: 0.4716 - val_loss: 1.5901 - val_accuracy: 0.4383\nEpoch 22/1000\n40000/40000 - 8s - loss: 1.4610 - accuracy: 0.4743 - val_loss: 1.5963 - val_accuracy: 0.4322\nEpoch 23/1000\n40000/40000 - 9s - loss: 1.4477 - accuracy: 0.4800 - val_loss: 1.5973 - val_accuracy: 0.4416\nEpoch 24/1000\n40000/40000 - 9s - loss: 1.4372 - accuracy: 0.4816 - val_loss: 1.5806 - val_accuracy: 0.4460\nEpoch 25/1000\n40000/40000 - 9s - loss: 1.4244 - accuracy: 0.4869 - val_loss: 1.6046 - val_accuracy: 0.4315\nEpoch 26/1000\n40000/40000 - 9s - loss: 1.4181 - accuracy: 0.4916 - val_loss: 1.5774 - val_accuracy: 0.4432\nEpoch 27/1000\n40000/40000 - 9s - loss: 1.4061 - accuracy: 0.4935 - val_loss: 1.6122 - val_accuracy: 0.4334\nEpoch 28/1000\n40000/40000 - 9s - loss: 1.3915 - accuracy: 0.5017 - val_loss: 1.5911 - val_accuracy: 0.4436\nEpoch 29/1000\n40000/40000 - 8s - loss: 1.3849 - accuracy: 0.5062 - val_loss: 1.5910 - val_accuracy: 0.4420\nEpoch 30/1000\n40000/40000 - 8s - loss: 1.3758 - accuracy: 0.5080 - val_loss: 1.6169 - val_accuracy: 0.4390\nEpoch 31/1000\n40000/40000 - 8s - loss: 1.3647 - accuracy: 0.5109 - val_loss: 1.6019 - val_accuracy: 0.4404\nEpoch 32/1000\n40000/40000 - 8s - loss: 1.3549 - accuracy: 0.5165 - val_loss: 1.6214 - val_accuracy: 0.4353\nEpoch 33/1000\n40000/40000 - 8s - loss: 1.3476 - accuracy: 0.5170 - val_loss: 1.6408 - val_accuracy: 0.4303\nEpoch 34/1000\n40000/40000 - 8s - loss: 1.3355 - accuracy: 0.5221 - val_loss: 1.6085 - val_accuracy: 0.4402\nEpoch 35/1000\n40000/40000 - 9s - loss: 1.3299 - accuracy: 0.5233 - val_loss: 1.6018 - val_accuracy: 0.4459\nEpoch 36/1000\n40000/40000 - 9s - loss: 1.3217 - accuracy: 0.5264 - val_loss: 1.5924 - val_accuracy: 0.4513\nEpoch 37/1000\n40000/40000 - 9s - loss: 1.3126 - accuracy: 0.5307 - val_loss: 1.5803 - val_accuracy: 0.4476\nEpoch 38/1000\n40000/40000 - 8s - loss: 1.3048 - accuracy: 0.5354 - val_loss: 1.6024 - val_accuracy: 0.4485\nEpoch 39/1000\n40000/40000 - 8s - loss: 1.2957 - accuracy: 0.5340 - val_loss: 1.6054 - val_accuracy: 0.4504\nEpoch 40/1000\n40000/40000 - 9s - loss: 1.2898 - accuracy: 0.5401 - val_loss: 1.6126 - val_accuracy: 0.4430\nEpoch 41/1000\n40000/40000 - 8s - loss: 1.2798 - accuracy: 0.5418 - val_loss: 1.6145 - val_accuracy: 0.4548\nEpoch 42/1000\n40000/40000 - 8s - loss: 1.2713 - accuracy: 0.5479 - val_loss: 1.5964 - val_accuracy: 0.4518\nEpoch 43/1000\n40000/40000 - 8s - loss: 1.2635 - accuracy: 0.5489 - val_loss: 1.6106 - val_accuracy: 0.4475\nEpoch 44/1000\n40000/40000 - 8s - loss: 1.2556 - accuracy: 0.5538 - val_loss: 1.6064 - val_accuracy: 0.4519\nEpoch 45/1000\n40000/40000 - 9s - loss: 1.2499 - accuracy: 0.5514 - val_loss: 1.6181 - val_accuracy: 0.4463\nEpoch 46/1000\n40000/40000 - 8s - loss: 1.2409 - accuracy: 0.5568 - val_loss: 1.6112 - val_accuracy: 0.4565\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f20ced265c0>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# create and train a basic DNN model\n",
    "model = create_dnn_model(lr=5e-5)\n",
    "\n",
    "model_name = \"cifar10_dnn\"\n",
    "model_filepath = f\"models/{model_name}.h5\"\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(keras.callbacks.EarlyStopping(patience=20))\n",
    "callbacks.append(keras.callbacks.TensorBoard(log_dir=generate_log_dir(model_name)))\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint(model_filepath, save_best_only=True))\n",
    "\n",
    "model.fit(X_train_small, y_train_small, validation_data=(X_val, y_val), epochs=1000, callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 1s 67us/sample - loss: 1.5774 - accuracy: 0.4432\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1.5774483856201171, 0.4432]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/cifar10_dnn.h5\")\n",
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest validation loss reaches 44.3% validation accuracy. It took 46 epochs to reach this result. Each epoch took about 9s on my machine.\n",
    "\n",
    "Let's see if Batch Normalization will improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 40000 samples, validate on 10000 samples\nEpoch 1/1000\n40000/40000 - 25s - loss: 2.1563 - accuracy: 0.2367 - val_loss: 1.8890 - val_accuracy: 0.3200\nEpoch 2/1000\n40000/40000 - 20s - loss: 1.8734 - accuracy: 0.3256 - val_loss: 1.7508 - val_accuracy: 0.3692\nEpoch 3/1000\n40000/40000 - 19s - loss: 1.7729 - accuracy: 0.3629 - val_loss: 1.6817 - val_accuracy: 0.3964\nEpoch 4/1000\n40000/40000 - 18s - loss: 1.7079 - accuracy: 0.3895 - val_loss: 1.6201 - val_accuracy: 0.4185\nEpoch 5/1000\n40000/40000 - 19s - loss: 1.6550 - accuracy: 0.4089 - val_loss: 1.5821 - val_accuracy: 0.4375\nEpoch 6/1000\n40000/40000 - 18s - loss: 1.6121 - accuracy: 0.4264 - val_loss: 1.5470 - val_accuracy: 0.4446\nEpoch 7/1000\n40000/40000 - 19s - loss: 1.5734 - accuracy: 0.4396 - val_loss: 1.5181 - val_accuracy: 0.4570\nEpoch 8/1000\n40000/40000 - 19s - loss: 1.5430 - accuracy: 0.4531 - val_loss: 1.5041 - val_accuracy: 0.4583\nEpoch 9/1000\n40000/40000 - 20s - loss: 1.5153 - accuracy: 0.4602 - val_loss: 1.4955 - val_accuracy: 0.4620\nEpoch 10/1000\n40000/40000 - 20s - loss: 1.4865 - accuracy: 0.4705 - val_loss: 1.4754 - val_accuracy: 0.4742\nEpoch 11/1000\n40000/40000 - 18s - loss: 1.4622 - accuracy: 0.4819 - val_loss: 1.4616 - val_accuracy: 0.4762\nEpoch 12/1000\n40000/40000 - 18s - loss: 1.4399 - accuracy: 0.4897 - val_loss: 1.4465 - val_accuracy: 0.4887\nEpoch 13/1000\n40000/40000 - 18s - loss: 1.4151 - accuracy: 0.4967 - val_loss: 1.4466 - val_accuracy: 0.4856\nEpoch 14/1000\n40000/40000 - 18s - loss: 1.3933 - accuracy: 0.5055 - val_loss: 1.4306 - val_accuracy: 0.4894\nEpoch 15/1000\n40000/40000 - 18s - loss: 1.3745 - accuracy: 0.5143 - val_loss: 1.4306 - val_accuracy: 0.4927\nEpoch 16/1000\n40000/40000 - 18s - loss: 1.3583 - accuracy: 0.5172 - val_loss: 1.4226 - val_accuracy: 0.4908\nEpoch 17/1000\n40000/40000 - 18s - loss: 1.3375 - accuracy: 0.5260 - val_loss: 1.4214 - val_accuracy: 0.4960\nEpoch 18/1000\n40000/40000 - 18s - loss: 1.3251 - accuracy: 0.5306 - val_loss: 1.4107 - val_accuracy: 0.4992\nEpoch 19/1000\n40000/40000 - 18s - loss: 1.3088 - accuracy: 0.5372 - val_loss: 1.4005 - val_accuracy: 0.5006\nEpoch 20/1000\n40000/40000 - 18s - loss: 1.2910 - accuracy: 0.5422 - val_loss: 1.4089 - val_accuracy: 0.4996\nEpoch 21/1000\n40000/40000 - 18s - loss: 1.2794 - accuracy: 0.5451 - val_loss: 1.3895 - val_accuracy: 0.5081\nEpoch 22/1000\n40000/40000 - 18s - loss: 1.2631 - accuracy: 0.5533 - val_loss: 1.4062 - val_accuracy: 0.5116\nEpoch 23/1000\n40000/40000 - 18s - loss: 1.2602 - accuracy: 0.5525 - val_loss: 1.3998 - val_accuracy: 0.5094\nEpoch 24/1000\n40000/40000 - 18s - loss: 1.2429 - accuracy: 0.5588 - val_loss: 1.4060 - val_accuracy: 0.4994\nEpoch 25/1000\n40000/40000 - 18s - loss: 1.2268 - accuracy: 0.5682 - val_loss: 1.3966 - val_accuracy: 0.5090\nEpoch 26/1000\n40000/40000 - 18s - loss: 1.2270 - accuracy: 0.5671 - val_loss: 1.4134 - val_accuracy: 0.5053\nEpoch 27/1000\n40000/40000 - 18s - loss: 1.2064 - accuracy: 0.5728 - val_loss: 1.4131 - val_accuracy: 0.5042\nEpoch 28/1000\n40000/40000 - 18s - loss: 1.2032 - accuracy: 0.5743 - val_loss: 1.4089 - val_accuracy: 0.5013\nEpoch 29/1000\n40000/40000 - 18s - loss: 1.1816 - accuracy: 0.5814 - val_loss: 1.4222 - val_accuracy: 0.4998\nEpoch 30/1000\n40000/40000 - 18s - loss: 1.1701 - accuracy: 0.5872 - val_loss: 1.4048 - val_accuracy: 0.5099\nEpoch 31/1000\n40000/40000 - 18s - loss: 1.1672 - accuracy: 0.5864 - val_loss: 1.4203 - val_accuracy: 0.5045\nEpoch 32/1000\n40000/40000 - 18s - loss: 1.1610 - accuracy: 0.5884 - val_loss: 1.3961 - val_accuracy: 0.5145\nEpoch 33/1000\n40000/40000 - 18s - loss: 1.1474 - accuracy: 0.5934 - val_loss: 1.4214 - val_accuracy: 0.5020\nEpoch 34/1000\n40000/40000 - 18s - loss: 1.1396 - accuracy: 0.5942 - val_loss: 1.4124 - val_accuracy: 0.5124\nEpoch 35/1000\n40000/40000 - 18s - loss: 1.1255 - accuracy: 0.5999 - val_loss: 1.4017 - val_accuracy: 0.5139\nEpoch 36/1000\n40000/40000 - 18s - loss: 1.1191 - accuracy: 0.6030 - val_loss: 1.4149 - val_accuracy: 0.5084\nEpoch 37/1000\n40000/40000 - 18s - loss: 1.1111 - accuracy: 0.6069 - val_loss: 1.4135 - val_accuracy: 0.5094\nEpoch 38/1000\n40000/40000 - 18s - loss: 1.0965 - accuracy: 0.6075 - val_loss: 1.4205 - val_accuracy: 0.5075\nEpoch 39/1000\n40000/40000 - 18s - loss: 1.0946 - accuracy: 0.6131 - val_loss: 1.4320 - val_accuracy: 0.5061\nEpoch 40/1000\n40000/40000 - 18s - loss: 1.0822 - accuracy: 0.6176 - val_loss: 1.4287 - val_accuracy: 0.5053\nEpoch 41/1000\n40000/40000 - 19s - loss: 1.0858 - accuracy: 0.6153 - val_loss: 1.4045 - val_accuracy: 0.5204\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f1ffcc43cc0>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# create and train a DNN model with Batch Normalization\n",
    "model = create_dnn_model(lr=1e-4, use_batch_norm=True)\n",
    "\n",
    "model_name = \"cifar10_dnn_bn\"\n",
    "model_filepath = f\"models/{model_name}.h5\"\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(keras.callbacks.EarlyStopping(patience=20))\n",
    "callbacks.append(keras.callbacks.TensorBoard(log_dir=generate_log_dir(model_name)))\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint(model_filepath, save_best_only=True))\n",
    "\n",
    "model.fit(X_train_small, y_train_small, validation_data=(X_val, y_val), epochs=1000, callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 1s 118us/sample - loss: 1.3895 - accuracy: 0.5081\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1.3895051637649536, 0.5081]"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/cifar10_dnn_bn.h5\")\n",
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the lowest validation loss has 50.8% validation accuracy. It took 41 epochs with each epoch taking about 18 seconds on my machine. Batch Norm added more trainable variables so it is taking longer time per epoch. In terms of convergence, both models seem to be stuck in local optima.\n",
    "\n",
    "Let's see if using SELU and LeCun normal initialization will improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SELU requires that the input features are standardized\n",
    "std_scaler = StandardScaler()\n",
    "X_train_small_scaled = X_train_small.reshape(X_train_small.shape[0], -1)    # flatten\n",
    "X_train_small_scaled = std_scaler.fit_transform(X_train_small_scaled)\n",
    "X_val_scaled = X_val.reshape(X_val.shape[0], -1)                            # flatten\n",
    "X_val_scaled = std_scaler.transform(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 40000 samples, validate on 10000 samples\nEpoch 1/1000\n40000/40000 - 11s - loss: 1.8826 - accuracy: 0.3271 - val_loss: 1.7336 - val_accuracy: 0.3759\nEpoch 2/1000\n40000/40000 - 9s - loss: 1.6556 - accuracy: 0.4076 - val_loss: 1.6553 - val_accuracy: 0.4166\nEpoch 3/1000\n40000/40000 - 9s - loss: 1.5541 - accuracy: 0.4470 - val_loss: 1.5867 - val_accuracy: 0.4342\nEpoch 4/1000\n40000/40000 - 9s - loss: 1.4836 - accuracy: 0.4717 - val_loss: 1.5727 - val_accuracy: 0.4418\nEpoch 5/1000\n40000/40000 - 9s - loss: 1.4279 - accuracy: 0.4904 - val_loss: 1.5561 - val_accuracy: 0.4485\nEpoch 6/1000\n40000/40000 - 9s - loss: 1.3777 - accuracy: 0.5077 - val_loss: 1.5371 - val_accuracy: 0.4551\nEpoch 7/1000\n40000/40000 - 9s - loss: 1.3303 - accuracy: 0.5283 - val_loss: 1.5241 - val_accuracy: 0.4656\nEpoch 8/1000\n40000/40000 - 10s - loss: 1.2848 - accuracy: 0.5424 - val_loss: 1.5331 - val_accuracy: 0.4676\nEpoch 9/1000\n40000/40000 - 9s - loss: 1.2444 - accuracy: 0.5557 - val_loss: 1.5321 - val_accuracy: 0.4705\nEpoch 10/1000\n40000/40000 - 9s - loss: 1.2058 - accuracy: 0.5707 - val_loss: 1.5330 - val_accuracy: 0.4715\nEpoch 11/1000\n40000/40000 - 10s - loss: 1.1708 - accuracy: 0.5832 - val_loss: 1.5350 - val_accuracy: 0.4758\nEpoch 12/1000\n40000/40000 - 10s - loss: 1.1304 - accuracy: 0.6003 - val_loss: 1.5420 - val_accuracy: 0.4776\nEpoch 13/1000\n40000/40000 - 9s - loss: 1.1001 - accuracy: 0.6115 - val_loss: 1.5472 - val_accuracy: 0.4806\nEpoch 14/1000\n40000/40000 - 9s - loss: 1.0662 - accuracy: 0.6206 - val_loss: 1.5782 - val_accuracy: 0.4774\nEpoch 15/1000\n40000/40000 - 9s - loss: 1.0335 - accuracy: 0.6337 - val_loss: 1.5879 - val_accuracy: 0.4747\nEpoch 16/1000\n40000/40000 - 9s - loss: 1.0042 - accuracy: 0.6441 - val_loss: 1.6074 - val_accuracy: 0.4766\nEpoch 17/1000\n40000/40000 - 9s - loss: 0.9753 - accuracy: 0.6532 - val_loss: 1.6338 - val_accuracy: 0.4769\nEpoch 18/1000\n40000/40000 - 9s - loss: 0.9456 - accuracy: 0.6642 - val_loss: 1.6403 - val_accuracy: 0.4777\nEpoch 19/1000\n40000/40000 - 9s - loss: 0.9150 - accuracy: 0.6746 - val_loss: 1.6728 - val_accuracy: 0.4683\nEpoch 20/1000\n40000/40000 - 10s - loss: 0.8903 - accuracy: 0.6856 - val_loss: 1.6705 - val_accuracy: 0.4745\nEpoch 21/1000\n40000/40000 - 9s - loss: 0.8631 - accuracy: 0.6949 - val_loss: 1.7178 - val_accuracy: 0.4753\nEpoch 22/1000\n40000/40000 - 9s - loss: 0.8361 - accuracy: 0.7048 - val_loss: 1.7251 - val_accuracy: 0.4761\nEpoch 23/1000\n40000/40000 - 9s - loss: 0.8070 - accuracy: 0.7139 - val_loss: 1.7601 - val_accuracy: 0.4832\nEpoch 24/1000\n40000/40000 - 9s - loss: 0.7838 - accuracy: 0.7235 - val_loss: 1.8075 - val_accuracy: 0.4723\nEpoch 25/1000\n40000/40000 - 9s - loss: 0.7563 - accuracy: 0.7333 - val_loss: 1.8369 - val_accuracy: 0.4737\nEpoch 26/1000\n40000/40000 - 9s - loss: 0.7348 - accuracy: 0.7383 - val_loss: 1.8961 - val_accuracy: 0.4681\nEpoch 27/1000\n40000/40000 - 9s - loss: 0.7067 - accuracy: 0.7513 - val_loss: 1.9139 - val_accuracy: 0.4691\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f1ead0c9550>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# create and train a DNN model with SELU and LeCun normal initialization\n",
    "model = create_dnn_model(lr=5e-5, use_batch_norm=False, activation=\"selu\", kernel_initializer=\"lecun_normal\", flatten=False)\n",
    "\n",
    "model_name = \"cifar10_dnn_selu\"\n",
    "model_filepath = f\"models/{model_name}.h5\"\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(keras.callbacks.EarlyStopping(patience=20))\n",
    "callbacks.append(keras.callbacks.TensorBoard(log_dir=generate_log_dir(model_name)))\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint(model_filepath, save_best_only=True))\n",
    "\n",
    "model.fit(X_train_small_scaled, y_train_small, validation_data=(X_val_scaled, y_val), epochs=1000, callbacks=callbacks, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 1s 125us/sample - loss: 1.5241 - accuracy: 0.4656\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1.5240588359832763, 0.4656]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/cifar10_dnn_selu.h5\")\n",
    "model.evaluate(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SELU and LeCun normal initialization produced a worse model than Batch Norm but it is slightly better than the original (ELU) model with a validation accuracy of 46.6%."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bit5171d068c3054b8f80dd7b7dde0249a4",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}